{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "df = spark.read.csv(\"s3://rumors3/bist_live_daily/bist_live_2024-04-01.csv\",header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lit,regexp_replace, col\n",
        "from datetime import datetime, timedelta\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import col,expr,format_number\n",
        "from pyspark.sql.functions import col, lit, min\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Read CSV Example\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "df1 = spark.read.csv(\"s3://rumors3/bist_live_daily/bist_live_2024-03-28.csv\", header=True, inferSchema=True)\n",
        "df1 = df1.withColumn('18:00', df1['17:00'])\n",
        "\n",
        "df2 = spark.read.csv(\"s3://rumors3/bist_live_daily/bist_live_2024-03-27.csv\", header=True, inferSchema=True)\n",
        "df2 = df2.withColumn('17:00', df2['16:00'])\n",
        "df2 = df2.withColumn('18:00', df2['16:00'])\n",
        "\n",
        "df1 = df1.withColumn('Date', lit('2024-03-28'))\n",
        "df2 = df2.withColumn('Date', lit('2024-03-27'))\n",
        "\n",
        "df3 = spark.read.csv(\"s3://rumors3/bist_live_daily/bist_live_2024-04-01.csv\", header=True, inferSchema=True)\n",
        "df3 = df3.withColumn('Date', lit('2024-04-01'))\n",
        "\n",
        "df4 = spark.read.csv(\"s3://rumors3/bist_live_daily/bist_live_2024-03-29.csv\", header=True, inferSchema=True)\n",
        "df4 = df4.withColumn('Date', lit('2024-03-29'))\n",
        "\n",
        "union_df = df1.union(df2).union(df3).union(df4)\n",
        "\n",
        "melted_df = union_df.selectExpr(\"stock_name\", \"Date\",\"stack(8, '10:00', `10:00`, '11:00', `11:00`, '12:00', `12:00`, '13:00', `13:00`, '14:00', `14:00`, '15:00', `15:00`, '16:00', `16:00`, '17:00', `17:00`) as (hour, value)\")\n",
        "\n",
        "stock_stats_df = melted_df.groupBy(\"stock_name\",\"Date\").agg(\n",
        "    expr(\"min(value) as min_value\"),\n",
        "    expr(\"max(value) as max_value\"),\n",
        "    format_number(expr(\"avg(value)\"), 2).alias(\"avg_value\"),\n",
        "    format_number(expr(\"stddev(value)\"), 2).alias(\"std_value\")\n",
        ")\n",
        "\n",
        "stock_stats_df.show()\n",
        "union_df.show()\n",
        "\n",
        "output_path = 's3://rumors3/union_df.parquet'\n",
        "output_path2 = 's3://rumors3/stock_stats_df.parquet'\n",
        "df_single_partition = union_df.coalesce(1)\n",
        "df_single_partition.write.parquet(output_path)\n",
        "\n",
        "stock_stats_df_single = stock_stats_df.coalesce(1)\n",
        "stock_stats_df_single.write.parquet(output_path2)\n",
        "\n",
        "# Stop SparkSession\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "df1 = spark.read.csv(\"s3://rumors3/bist_live_daily/bist_live_2024-03-28.csv\", header=True, inferSchema=True)\n",
        "\n",
        "df1.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lit,regexp_replace, col\n",
        "from datetime import datetime, timedelta\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Read JSON Data\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Calculate the date for yesterday\n",
        "todate = datetime.today().date()\n",
        "\n",
        "days = ['2024-04-03,'2024-04-02','2024-04-01','2024-03-29','2024-03-28','2024-03-27','2024-03-26']\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Empty DataFrame Creation\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "schema_analysis = StructType([\n",
        "    StructField(\"comment\", StringType(), True),\n",
        "    StructField(\"date\", DateType(), True),\n",
        "    StructField(\"disc_index\", IntegerType(), True),\n",
        "    StructField(\"ticker\", StringType(), True)\n",
        "])\n",
        "\n",
        "analysis_df = spark.createDataFrame([], schema_analysis)\n",
        "\n",
        "\n",
        "for i in days:\n",
        "    analysis_path = f\"s3://rumors3/KAP/KAP_analysis/analysis_results_{i}.json\"\n",
        "\n",
        "    df = spark.read \\\n",
        "        .option(\"multiline\", \"true\") \\\n",
        "        .json(analysis_path)\n",
        "    analysis_df = analysis_df.union(df)\n",
        "\n",
        "schema_statement = StructType([\n",
        "    StructField(\"company_name\", StringType(), True),\n",
        "    StructField(\"disc_index\", IntegerType(), True),\n",
        "    StructField(\"text\", StringType(), True)\n",
        "])\n",
        "\n",
        "statement_df = spark.createDataFrame([], schema_statement)\n",
        "for i in days:\n",
        "    statement_path = f\"s3://rumors3/KAP/KAP_statements/statement_{i}.json\"\n",
        "\n",
        "    df = spark.read \\\n",
        "        .option(\"multiline\", \"true\") \\\n",
        "        .json(statement_path)\n",
        "    statement_df = statement_df.union(df)\n",
        "    \n",
        "analysis_df_alias = analysis_df.alias(\"analysis\")\n",
        "statement_df_alias = statement_df.alias(\"statement\")\n",
        "\n",
        "\n",
        "joined_df = analysis_df_alias.join(statement_df_alias, col(\"analysis.disc_index\") == col(\"statement.disc_index\"), \"inner\")\n",
        "joined_df = joined_df.drop(col(\"statement.disc_index\"))\n",
        "joined_df = joined_df.drop('ticker')\n",
        "\n",
        "\n",
        "\n",
        "rdd = joined_df.rdd\n",
        "\n",
        "def replace_newline(row):\n",
        "    row_dict = row.asDict()\n",
        "    row_dict['comment'] = row_dict['comment'].replace(\"\\n\", \"\")\n",
        "    return row_dict\n",
        "\n",
        "# Apply the function to each row of the RDD\n",
        "new_rdd = rdd.map(replace_newline)\n",
        "\n",
        "new_rdd2 = new_rdd.map(lambda x: ((x['company_name'], x['date']), 1)) \\\n",
        "                 .reduceByKey(lambda x, y: x + y) \\\n",
        "                 .map(lambda x: (x[0][0], x[0][1], x[1]))\n",
        "\n",
        "count_df = spark.createDataFrame(new_rdd2, schema=['company_name','date','count'])\n",
        "\n",
        "# Convert the resulting RDD back to a DataFrame\n",
        "new_df = spark.createDataFrame(new_rdd, schema=joined_df.schema)\n",
        "\n",
        "# Show the DataFrame\n",
        "new_df.show()\n",
        "count_df.show()\n",
        "\n",
        "output_path = 's3://rumors3/kap_analysis.parquet'\n",
        "output_path2 = 's3://rumors3/kap_count.parquet'\n",
        "df_single_partition = new_df.coalesce(1)\n",
        "df_single_partition.write.parquet(output_path)\n",
        "\n",
        "df_single_partition2 = count_df.coalesce(1)\n",
        "df_single_partition2.write.parquet(output_path2)\n",
        "\n",
        "\n",
        "# Stop SparkSession\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "\n",
        "output_path = 's3://rumors3/kap_analysis.parquet'\n",
        "output_path2 = 's3://rumors3/kap_count.parquet'\n",
        "df_single_partition = new_df.coalesce(1)\n",
        "df_single_partition.write.parquet(output_path)\n",
        "\n",
        "df_single_partition2 = count_df.coalesce(1)\n",
        "df_single_partition2.write.parquet(output_path2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lit, regexp_replace, col\n",
        "from datetime import datetime, timedelta\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Read Comments Data\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Calculate the date for yesterday\n",
        "todate = datetime.today().date()\n",
        "\n",
        "\n",
        "days = ['2024-04-03,'2024-04-02','2024-04-01','2024-03-29','2024-03-28','2024-03-27','2024-03-26']\n",
        "\n",
        "schema_comments = StructType([\n",
        "    StructField(\"Stock\", StringType(), True),\n",
        "    StructField(\"User\", StringType(), True),\n",
        "    StructField(\"Comment\", StringType(), True),\n",
        "    StructField(\"Comment Time\", StringType(), True),  # Change StringType to DateType\n",
        "    StructField(\"Date\", DateType(), True)\n",
        "])\n",
        "\n",
        "comments_df = spark.createDataFrame([], schema_comments)\n",
        "\n",
        "for i in days:\n",
        "    comments_path = f\"s3://rumors3/bist_comments/bist_comments_{i}.json\"\n",
        "\n",
        "    df = spark.read \\\n",
        "        .option(\"multiline\", \"true\") \\\n",
        "        .schema(schema_comments) \\\n",
        "        .json(comments_path)\n",
        "    comments_df = comments_df.union(df)\n",
        "\n",
        "comments_df = comments_df.drop('Comment Time', 'User')\n",
        "\n",
        "rdd = comments_df.rdd.map(lambda x: ((x['Stock'], x['Date']), 1)) \\\n",
        "                     .reduceByKey(lambda x, y: x + y) \\\n",
        "                     .map(lambda x: (x[0][0], x[0][1], x[1]))\n",
        "\n",
        "\n",
        "rdd2 = comments_df.rdd.map(lambda x: (x['Comment']))\n",
        "\n",
        "top_comments = rdd.filter(lambda x: x[1] == todate).sortBy(lambda x: x[2],ascending = False)\n",
        "top_5_elements = top_comments.take(5)\n",
        "\n",
        "# Extracting comments\n",
        "comments_rdd = comments_df.rdd.map(lambda x: x['Comment'].lower().split())\n",
        "\n",
        "# Filter out stop words\n",
        "turkish_stop_words = ['acaba', 'ama', 'aslında', 'az', 'bazı', 'belki', 'biri', 'birkaç', 'birşey', 'biz', 'bu',\n",
        "                      'çok', 'çünkü', 'da', 'daha', 'de', 'defa', 'diye', 'eğer', 'en', 'gibi', 'hem', 'hep',\n",
        "                      'hepsi', 'her', 'hiç', 'için', 'ile', 'ise', 'kez', 'ki', 'kim', 'mı', 'mu', 'mü', 'nasıl',\n",
        "                      'ne', 'neden', 'nerde', 'nerede', 'nereye', 'niçin', 'niye', 'o', 'sanki', 'şey', 'siz', 'şu',\n",
        "                      'tüm', 've', 'veya', 'ya', 'yani','bir','var','cok','mi','kadar','bi','1','2','3','e','a','5','10']\n",
        "filtered_comments_rdd = comments_rdd.map(lambda words: [word for word in words if word not in turkish_stop_words])\n",
        "\n",
        "# Count word occurrences\n",
        "word_counts = filtered_comments_rdd.flatMap(lambda words: [(word, 1) for word in words]) \\\n",
        "    .reduceByKey(lambda a, b: a + b) \\\n",
        "    .sortBy(lambda x: x[1], ascending=False) \\\n",
        "    .collect()\n",
        "\n",
        "\n",
        "# Convert RDD to DataFrame\n",
        "count_df = spark.createDataFrame(rdd, ['Stock', 'Date', 'CommentCount']).orderBy('Date','CommentCount',ascending=False)\n",
        "top_5_elements_df = spark.createDataFrame(top_5_elements, ['Stock', 'Date', 'CommentCount'])\n",
        "word_counts_df = spark.createDataFrame(word_counts, ['Word', 'Count'])\n",
        "\n",
        "top_5_elements_df.show()\n",
        "count_df.show()\n",
        "word_counts_df.show()\n",
        "\n",
        "output_path = 's3://rumors3/comments_top_5.parquet'\n",
        "output_path2 = 's3://rumors3/comments_count.parquet'\n",
        "output_path3 = 's3://rumors3/comments_word_count.parquet'\n",
        "\n",
        "df_single_partition = top_5_elements_df.coalesce(1)\n",
        "df_single_partition.write.parquet(output_path)\n",
        "\n",
        "df_single_partition2 = count_df.coalesce(1)\n",
        "df_single_partition2.write.parquet(output_path2)\n",
        "\n",
        "df_single_partition3 = word_counts_df.coalesce(1)\n",
        "df_single_partition3.write.parquet(output_path3)\n",
        "\n",
        "spark.sparkContext.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    },
    "name": "Test-process"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
